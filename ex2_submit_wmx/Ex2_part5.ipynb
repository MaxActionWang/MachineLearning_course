{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ICU survivor prediction\n",
    "# Machine Learning, Exercise 2\n",
    "王敏行 id:2018012386 wangmx18@mails.tsinghua.edu.cn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp.4 MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "ts = pd.read_csv('data1forEx1to4/test1_icu_data.csv')\n",
    "tr = pd.read_csv('data1forEx1to4/train1_icu_data.csv')\n",
    "Y_tr = pd.read_csv('data1forEx1to4/train1_icu_label.csv')\n",
    "Y_ts = pd.read_csv('data1forEx1to4/test1_icu_label.csv')\n",
    "\n",
    "x_ts = np.array(ts)\n",
    "x_tr = np.array(tr)\n",
    "y_ts = np.array(Y_ts).ravel()\n",
    "y_tr = np.array(Y_tr).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codes framework is built by TA in the course NMDA.\n",
    "# refer to this program: https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/\n",
    "# Minxing Wang @ Dec 2021\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Network(object):\n",
    "\n",
    "    def __init__(self, hidden_size, input_size = 108, output_size = 2, std = 1e-4):\n",
    "        self.params = {}\n",
    "        self.params['W1'] = np.random.randn(input_size, hidden_size)*std\n",
    "        self.params['W2'] = np.random.randn(hidden_size, output_size)*std\n",
    "        self.params['b1']=np.zeros(hidden_size,)#写成一维array，可以在forward_pass里面方便和二维数组加和\n",
    "        self.params['b2']=np.zeros(output_size,)       \n",
    "        return\n",
    "    \n",
    "    def relu(self, x):\n",
    "        return np.clip(x,0,np.inf)\n",
    "    \n",
    "    def softmax(self,x):\n",
    "        return np.exp(x)/np.sum(np.exp(x), axis=-1, keepdims=True)\n",
    "\n",
    "    def cross_entropy(self, y_pred, y):\n",
    "        # prevent taking the log of 0\n",
    "        eps = np.finfo(float).eps\n",
    "        return -np.sum(y * np.log(y_pred+eps))\n",
    "\n",
    "    def forward_pass(self, X, y = None, wd_decay = 0.0):\n",
    "    \n",
    "        loss = None\n",
    "        predict = None\n",
    "        self.var = {}\n",
    "        self.var['z1'] = np.dot(X, self.params['W1'])+self.params['b1']\n",
    "        self.var['h'] = self.relu(self.var['z1'])\n",
    "        self.var['c'] = np.dot(self.var['h'], self.params['W2']) + self.params['b2']\n",
    "        self.var['pred'] = self.softmax(self.var['c'])\n",
    "        ###\n",
    "\n",
    "        \n",
    "        if y is None:\n",
    "\n",
    "            predict = np.argmax(self.var['pred'], axis=1)#求每一行的最大值的索引\n",
    "            return predict\n",
    "        else:\n",
    "            n = X.shape[0]\n",
    "            m = self.var['c'].shape[1]\n",
    "            y_true_onehot = np.zeros((n,m))\n",
    "            for i in range(n):\n",
    "                y_true_onehot[i,y[i]] = 1\n",
    "            loss = self.cross_entropy(self.var['pred'], y_true_onehot) / n\n",
    "            for para in self.params:\n",
    "                loss += 0.5*wd_decay*((self.params[para]**2).sum())\n",
    "            return loss\n",
    "\n",
    "\n",
    "    def back_prop(self, X, y, wd_decay = 0.0):\n",
    "        grads = {}\n",
    "        self.var = {}\n",
    "        self.var['z1'] = np.dot(X, self.params['W1'])+self.params['b1']\n",
    "        self.var['h'] = self.relu(self.var['z1'])\n",
    "        self.var['c'] = np.dot(self.var['h'], self.params['W2']) + self.params['b2']\n",
    "        self.var['pred'] = self.softmax(self.var['c'])\n",
    "        grads = {}\n",
    "        n = X.shape[0]\n",
    "        m = self.var['pred'].shape[1]\n",
    "        y_true_onehot = np.zeros((n,m))\n",
    "        for i in range(n):\n",
    "            y_true_onehot[i,y[i]] = 1            \n",
    "        dy = self.var['pred']-y_true_onehot\n",
    "\n",
    "        grads['b2'] = np.sum(dy, axis=0, keepdims=True) / n + wd_decay*self.params['b2']\n",
    "        grads['W2'] = np.dot((self.var['h'].T), dy) / n + wd_decay*self.params['W2']\n",
    "        dz1 = np.dot(dy, self.params['W2'].T) * (self.var['h'] > 0)\n",
    "        grads['b1'] = np.sum(dz1, axis=0, keepdims=True) / n + wd_decay*self.params['b1']\n",
    "        grads['W1'] = np.dot(X.T, dz1) / n + wd_decay*self.params['W1']\n",
    "        return grads\n",
    " \n",
    "    def numerical_gradient(self, X, y, wd_decay = 0.0, delta = 1e-6):\n",
    "        grads = {}\n",
    "            \n",
    "        for param_name in self.params:\n",
    "            grads[param_name] = np.zeros(self.params[param_name].shape)\n",
    "            itx = np.nditer(self.params[param_name], flags=['multi_index'], op_flags=['readwrite'])\n",
    "            while not itx.finished:\n",
    "                idx = itx.multi_index\n",
    "                grads[param_name][idx] = 0\n",
    "                param_temp = self.params[param_name][idx]\n",
    "                self.params[param_name][idx] = param_temp + delta\n",
    "                loss_up = self.forward_pass(X,y,wd_decay=wd_decay)\n",
    "                self.params[param_name][idx] = param_temp - delta\n",
    "                loss_low = self.forward_pass(X,y,wd_decay=wd_decay)\n",
    "                grads[param_name][idx] = (loss_up - loss_low)/delta/2\n",
    "                self.params[param_name][idx] = param_temp  \n",
    "                \n",
    "                itx.iternext()\n",
    "        return grads\n",
    "    \n",
    "    def get_acc(self, X, y):\n",
    "        pred = self.forward_pass(X)\n",
    "        return np.mean(pred == y)\n",
    "    \n",
    "    def train(self, X, y, X_val, y_val,\n",
    "                learning_rate=0, lr_decay=1,\n",
    "                momentum=0, do_early_stopping=False, stopping_patience=0,\n",
    "                wd_decay=0, num_iters=10,\n",
    "                batch_size=4, verbose=False, print_every=10):\n",
    "\n",
    "        num_train = X.shape[0]\n",
    "        iterations_per_epoch = max(num_train // batch_size, 1)#应该是整除而不是除以\n",
    "\n",
    "        loss_history = []\n",
    "        acc_history = []\n",
    "        val_acc_history = []\n",
    "        val_loss_history = []\n",
    "        \n",
    "\n",
    "        epoch = 0\n",
    "        grad_last = {'W1':0,'W2':0,'b1':0,'b2':0}\n",
    "        grad_tmp={}\n",
    "        train_array = np.arange(num_train)\n",
    "        np.random.shuffle(train_array)\n",
    "\n",
    "        for it in range(num_iters):\n",
    "            batch_num = it % iterations_per_epoch\n",
    "            epoch = it //iterations_per_epoch\n",
    "            #learning_rate w/ or w/o decay:\n",
    "            learning_rate_tmp = learning_rate/(1+lr_decay*epoch)\n",
    "            # learning_rate_tmp = learning_rate\n",
    "            batch_idx = train_array[(batch_num * batch_size):min((batch_num + 1) * batch_size,num_train)]\n",
    "            X_batch = X[batch_idx]\n",
    "            y_batch = y[batch_idx]\n",
    "            grad_tmp = self.back_prop(X_batch, y_batch, wd_decay)\n",
    "            \n",
    "            for param_name in self.params:\n",
    "                self.params[param_name] = self.params[param_name] - learning_rate_tmp * (momentum * grad_last[param_name] + (1 - momentum) * grad_tmp[param_name])\n",
    "            grad_last = grad_tmp\n",
    "\n",
    "            loss = self.forward_pass(X_batch,y_batch)\n",
    "            val_loss = self.forward_pass(X_val,y_val)\n",
    "            loss_history.append(loss)\n",
    "            val_loss_history.append(val_loss)\n",
    "            # train_acc = self.get_acc(X_batch, y_batch)\n",
    "            # val_acc = self.get_acc(X_val, y_val)\n",
    "            # acc_history.append(train_acc)\n",
    "            # val_acc_history.append(val_acc)\n",
    "            \n",
    "            if verbose and it % print_every == 0:\n",
    "                print('iteration %d / %d: training loss %f val loss: %f' % (it, num_iters, loss, val_loss))\n",
    " \n",
    "            if it % iterations_per_epoch == 0:\n",
    "                \n",
    "                train_acc = self.get_acc(X_batch, y_batch)\n",
    "                val_acc = self.get_acc(X_val, y_val)\n",
    "                acc_history.append(train_acc)\n",
    "                val_acc_history.append(val_acc)\n",
    "\n",
    "            if do_early_stopping:\n",
    "                n_compare = 50\n",
    "                n1 = 2*n_compare+1\n",
    "                n2 = n_compare+1\n",
    "                if (np.mean(val_loss_history[-n1:-n2])-np.mean(val_loss_history[-n2:-1])<stopping_patience) and (it>n_compare*3):\n",
    "                    print('iteration %d / %d: training loss %f val loss: %f' % (it, num_iters, loss, val_loss))\n",
    "                    print('ITERATION STOPPED, it = %d' % (it))\n",
    "                    break\n",
    "\n",
    "\n",
    "        return {\n",
    "          'loss_history': loss_history,\n",
    "          'val_loss_history': val_loss_history,\n",
    "          'acc_history': acc_history,\n",
    "          'val_acc_history': val_acc_history,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 200: training loss 0.687501 val loss: 0.692321\n",
      "iteration 10 / 200: training loss 0.692359 val loss: 0.694276\n",
      "iteration 20 / 200: training loss 0.695147 val loss: 0.693849\n",
      "iteration 30 / 200: training loss 0.684529 val loss: 0.694607\n",
      "iteration 40 / 200: training loss 0.697467 val loss: 0.694567\n",
      "iteration 50 / 200: training loss 0.691635 val loss: 0.693200\n",
      "iteration 60 / 200: training loss 0.692490 val loss: 0.693478\n",
      "iteration 70 / 200: training loss 0.695560 val loss: 0.694063\n",
      "iteration 80 / 200: training loss 0.686981 val loss: 0.693777\n",
      "iteration 90 / 200: training loss 0.696314 val loss: 0.694042\n",
      "iteration 100 / 200: training loss 0.693209 val loss: 0.693149\n",
      "iteration 110 / 200: training loss 0.692532 val loss: 0.693419\n",
      "iteration 120 / 200: training loss 0.695551 val loss: 0.694059\n",
      "iteration 130 / 200: training loss 0.688880 val loss: 0.693403\n",
      "iteration 140 / 200: training loss 0.695583 val loss: 0.693748\n",
      "iteration 150 / 200: training loss 0.694048 val loss: 0.693194\n",
      "iteration 160 / 200: training loss 0.692552 val loss: 0.693397\n",
      "iteration 170 / 200: training loss 0.695437 val loss: 0.693998\n",
      "iteration 180 / 200: training loss 0.690296 val loss: 0.693240\n",
      "iteration 190 / 200: training loss 0.695171 val loss: 0.693599\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 10\n",
    "\n",
    "net = Network(hidden_size = hidden_size)\n",
    "\n",
    "stats = net.train(x_tr, y_tr, x_ts, y_ts,\n",
    "            learning_rate=0.5, momentum=0, wd_decay=0.02, lr_decay=0.5, \n",
    "            num_iters=200, batch_size=100,\n",
    "            do_early_stopping=False,\n",
    "            print_every=10, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = net.forward_pass(x_tr)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "93cb51f5991ec0b90525fa9a9f56cc67747d2aebc05662b8a7d03c580c444d66"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
